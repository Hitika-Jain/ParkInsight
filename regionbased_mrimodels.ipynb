{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19DYJ0gZgpgBk8oR7AHnUpOf8UUCEe9SY",
      "authorship_tag": "ABX9TyPOFtQKGOO8DCyEjiGiaJxU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hitika-Jain/ParkInsight/blob/main/regionbased_mrimodels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYRNnUFvCYz-",
        "outputId": "6b4b100d-7864-4857-c219-0e29f2ebf74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nilearn\n",
            "  Downloading nilearn-0.11.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.4.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from nilearn) (5.3.2)\n",
            "Requirement already satisfied: nibabel>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nilearn) (24.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.14.1)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel>=5.2.0->nilearn) (6.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel>=5.2.0->nilearn) (4.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.0->nilearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->nilearn) (1.17.0)\n",
            "Downloading nilearn-0.11.1-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nilearn\n",
            "Successfully installed nilearn-0.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "qgW9UHN7vctR",
        "outputId": "fd8076c6-58f6-4816-a830-8229babeb119"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34m_add_readme_to_default_data_locations\u001b[0m\u001b[1;34m]\u001b[0m Added README.md to \u001b[35m/root/\u001b[0m\u001b[95mnilearn_data\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">_add_readme_to_default_data_locations</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Added README.md to <span style=\"color: #800080; text-decoration-color: #800080\">/root/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">nilearn_data</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mget_dataset_dir\u001b[0m\u001b[1;34m]\u001b[0m Dataset created in \u001b[35m/root/nilearn_data/\u001b[0m\u001b[95mfsl\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">get_dataset_dir</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Dataset created in <span style=\"color: #800080; text-decoration-color: #800080\">/root/nilearn_data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">fsl</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mfetch_single_file\u001b[0m\u001b[1;34m]\u001b[0m Downloading data from \u001b[4;94mhttps://www.nitrc.org/frs/download.php/9902/HarvardOxford.tgz\u001b[0m \u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_single_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Downloading data from <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://www.nitrc.org/frs/download.php/9902/HarvardOxford.tgz</span> <span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34m_chunk_report_\u001b[0m\u001b[1;34m]\u001b[0m Downloaded \u001b[1;36m18063360\u001b[0m of \u001b[1;36m25716861\u001b[0m bytes \u001b[1m(\u001b[0m\u001b[1;36m70.2\u001b[0m%%,    \u001b[1;36m0.\u001b[0m4s remaining\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">_chunk_report_</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Downloaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18063360</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25716861</span> bytes <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70.2</span>%%,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>4s remaining<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mfetch_single_file\u001b[0m\u001b[1;34m]\u001b[0m  \u001b[33m...\u001b[0mdone. \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m seconds, \u001b[1;36m0\u001b[0m min\u001b[1m)\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">fetch_single_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span>  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>done. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> seconds, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> min<span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34muncompress_file\u001b[0m\u001b[1;34m]\u001b[0m Extracting data from \u001b[35m/root/nilearn_data/fsl/a7fc85ee019a9a4f8a037cba0214db4f/\u001b[0m\u001b[95mHarvardOxford.tgz...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">uncompress_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Extracting data from <span style=\"color: #800080; text-decoration-color: #800080\">/root/nilearn_data/fsl/a7fc85ee019a9a4f8a037cba0214db4f/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">HarvardOxford.tgz...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34muncompress_file\u001b[0m\u001b[1;34m]\u001b[0m .. done.\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">uncompress_file</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> .. done.\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-e2ec01e914ac>:38: FutureWarning: 'force_resample' will be set to 'True' by default in Nilearn 0.13.0.\n",
            "Use 'force_resample=True' to suppress this warning.\n",
            "  resampled_mask = resample_to_img(region_mask_img, brain_img, interpolation='nearest')\n",
            "/usr/local/lib/python3.11/dist-packages/nilearn/image/resampling.py:805: FutureWarning: From release 0.13.0 onwards, this function will, by default, copy the header of the input image to the output. Currently, the header is reset to the default Nifti1Header. To suppress this warning and use the new behavior, set `copy_header=True`.\n",
            "  return resample_img(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[x] Region Left_Caudate is empty, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-e2ec01e914ac>:38: FutureWarning: 'force_resample' will be set to 'True' by default in Nilearn 0.13.0.\n",
            "Use 'force_resample=True' to suppress this warning.\n",
            "  resampled_mask = resample_to_img(region_mask_img, brain_img, interpolation='nearest')\n",
            "/usr/local/lib/python3.11/dist-packages/nilearn/image/resampling.py:805: FutureWarning: From release 0.13.0 onwards, this function will, by default, copy the header of the input image to the output. Currently, the header is reset to the default Nifti1Header. To suppress this warning and use the new behavior, set `copy_header=True`.\n",
            "  return resample_img(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[x] Region Left_Putamen is empty, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-e2ec01e914ac>:38: FutureWarning: 'force_resample' will be set to 'True' by default in Nilearn 0.13.0.\n",
            "Use 'force_resample=True' to suppress this warning.\n",
            "  resampled_mask = resample_to_img(region_mask_img, brain_img, interpolation='nearest')\n",
            "/usr/local/lib/python3.11/dist-packages/nilearn/image/resampling.py:805: FutureWarning: From release 0.13.0 onwards, this function will, by default, copy the header of the input image to the output. Currently, the header is reset to the default Nifti1Header. To suppress this warning and use the new behavior, set `copy_header=True`.\n",
            "  return resample_img(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[x] Region Brain_Stem is empty, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-e2ec01e914ac>:38: FutureWarning: 'force_resample' will be set to 'True' by default in Nilearn 0.13.0.\n",
            "Use 'force_resample=True' to suppress this warning.\n",
            "  resampled_mask = resample_to_img(region_mask_img, brain_img, interpolation='nearest')\n",
            "/usr/local/lib/python3.11/dist-packages/nilearn/image/resampling.py:805: FutureWarning: From release 0.13.0 onwards, this function will, by default, copy the header of the input image to the output. Currently, the header is reset to the default Nifti1Header. To suppress this warning and use the new behavior, set `copy_header=True`.\n",
            "  return resample_img(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[✓] Saved: /content/voxels/Right_White_Matter/hc_176347_0.npy\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from nilearn import datasets\n",
        "from nilearn.image import resample_to_img\n",
        "from scipy.ndimage import zoom\n",
        "import os\n",
        "\n",
        "# CONFIGURATION\n",
        "npy_file_path = \"/content/drive/MyDrive/3D_Numpy_Volumes/hc/hc_176347.npy\"  # Change this\n",
        "output_dir = \"/content/voxels\"\n",
        "label = 0  # 0 for healthy, 1 for Parkinson's\n",
        "\n",
        "region_names = ['Left_Caudate', 'Left_Putamen', 'Brain_Stem', 'Right_White_Matter']\n",
        "regions_to_extract = [5, 6, 8, 12]  # IDs from Harvard-Oxford atlas\n",
        "\n",
        "# Create output directories\n",
        "for region in region_names:\n",
        "    os.makedirs(os.path.join(output_dir, region), exist_ok=True)\n",
        "\n",
        "# Load the brain volume (3D)\n",
        "brain_data = np.load(npy_file_path)\n",
        "\n",
        "# Convert to Nifti1Image with identity affine\n",
        "brain_img = nib.Nifti1Image(brain_data, affine=np.eye(4))\n",
        "\n",
        "# Load atlas\n",
        "atlas = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr50-1mm')\n",
        "atlas_img = nib.load(atlas.filename)\n",
        "atlas_data = atlas_img.get_fdata()\n",
        "atlas_affine = atlas_img.affine\n",
        "\n",
        "# Function to extract, resize, and save voxel region\n",
        "def extract_and_save(region_idx, region_name):\n",
        "    region_mask = atlas_data == region_idx\n",
        "    region_mask_img = nib.Nifti1Image(region_mask.astype(np.float32), affine=atlas_affine)\n",
        "\n",
        "    # Resample mask to brain image space\n",
        "    resampled_mask = resample_to_img(region_mask_img, brain_img, interpolation='nearest')\n",
        "    mask_data = resampled_mask.get_fdata()\n",
        "\n",
        "    # Apply mask\n",
        "    masked_region = brain_data * mask_data\n",
        "\n",
        "    # If region has any non-zero voxels\n",
        "    if np.any(masked_region):\n",
        "        x, y, z = np.where(masked_region != 0)\n",
        "        xmin, xmax = x.min(), x.max()\n",
        "        ymin, ymax = y.min(), y.max()\n",
        "        zmin, zmax = z.min(), z.max()\n",
        "\n",
        "        cropped = masked_region[xmin:xmax+1, ymin:ymax+1, zmin:zmax+1]\n",
        "\n",
        "        # Resize to 32x32x32\n",
        "        resized = zoom(cropped, (32 / cropped.shape[0], 32 / cropped.shape[1], 32 / cropped.shape[2]), order=1)\n",
        "\n",
        "        # Save\n",
        "        region_folder = os.path.join(output_dir, region_name)\n",
        "        base_name = os.path.splitext(os.path.basename(npy_file_path))[0]\n",
        "        save_path = os.path.join(region_folder, f\"{base_name}_{label}.npy\")\n",
        "        np.save(save_path, resized)\n",
        "\n",
        "        print(f\"[✓] Saved: {save_path}\")\n",
        "    else:\n",
        "        print(f\"[x] Region {region_name} is empty, skipping.\")\n",
        "\n",
        "# Process each region\n",
        "for idx, name in zip(regions_to_extract, region_names):\n",
        "    extract_and_save(idx, name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from nilearn import datasets\n",
        "from nilearn.image import resample_to_img, new_img_like, math_img\n",
        "from scipy.ndimage import zoom\n",
        "import os\n",
        "\n",
        "# ----------- CONFIG -------------\n",
        "npy_path = \"/content/drive/MyDrive/3D_Numpy_Volumes/pd/pd_101179.npy\"  # Change path\n",
        "subject_id = \"pd_101179\"  # Filename prefix\n",
        "label = 1  # 0 for healthy, 1 for Parkinson’s\n",
        "save_dir = \"/content/extracted_regions\"\n",
        "\n",
        "# Region IDs and names\n",
        "region_info = {\n",
        "    5: \"Left_Caudate\",\n",
        "    6: \"Left_Putamen\",\n",
        "    8: \"Brainstem\",\n",
        "    12: \"Right_White_Matter\"\n",
        "}\n",
        "# --------------------------------\n",
        "\n",
        "# Create output folders\n",
        "for region_name in region_info.values():\n",
        "    os.makedirs(os.path.join(save_dir, region_name), exist_ok=True)\n",
        "\n",
        "# Load .npy and wrap in Nifti with dummy affine\n",
        "brain_array = np.load(npy_path)\n",
        "# Remove all singleton dimensions\n",
        "brain_array = np.squeeze(brain_array)\n",
        "\n",
        "# Ensure we now have a 3D array\n",
        "if brain_array.ndim != 3:\n",
        "    raise ValueError(f\"❌ Still not 3D after squeezing: {brain_array.shape}\")\n",
        "affine = np.array([\n",
        "    [2, 0, 0, -90],\n",
        "    [0, 2, 0, -126],\n",
        "    [0, 0, 2, -72],\n",
        "    [0, 0, 0, 1]\n",
        "])\n",
        "brain_img = nib.Nifti1Image(brain_array, affine)\n",
        "\n",
        "# Load Harvard-Oxford atlas\n",
        "atlas = datasets.fetch_atlas_harvard_oxford('sub-maxprob-thr50-1mm')\n",
        "atlas_img = atlas.maps\n",
        "labels = atlas.labels\n",
        "\n",
        "# Resample brain to atlas space\n",
        "resampled_brain = resample_to_img(brain_img, atlas_img, interpolation='continuous', force_resample=True)\n",
        "\n",
        "# Iterate over regions\n",
        "for region_index, region_name in region_info.items():\n",
        "    print(f\"Processing Region {region_index}: {region_name}\")\n",
        "\n",
        "    atlas_data = atlas_img.get_fdata()\n",
        "    region_mask_data = (atlas_data == region_index).astype(np.uint8)\n",
        "    region_mask = new_img_like(atlas_img, region_mask_data)\n",
        "\n",
        "    region_volume_img = math_img(\"img1 * img2\", img1=resampled_brain, img2=region_mask)\n",
        "    region_volume = region_volume_img.get_fdata()\n",
        "\n",
        "    nonzero_voxels = np.count_nonzero(region_volume)\n",
        "    if nonzero_voxels == 0:\n",
        "        print(f\"⚠️ Region {region_name} is empty. Skipping.\\n---\")\n",
        "        continue\n",
        "\n",
        "    # Bounding box crop\n",
        "    coords = np.nonzero(region_volume)\n",
        "    zmin, zmax = np.min(coords[2]), np.max(coords[2])\n",
        "    ymin, ymax = np.min(coords[1]), np.max(coords[1])\n",
        "    xmin, xmax = np.min(coords[0]), np.max(coords[0])\n",
        "    region_crop = region_volume[xmin:xmax+1, ymin:ymax+1, zmin:zmax+1]\n",
        "    print(f\"  Original shape: {region_crop.shape}\")\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    region_crop = region_crop.astype(np.float32)\n",
        "    region_crop -= region_crop.min()\n",
        "    if region_crop.max() > 0:\n",
        "        region_crop /= region_crop.max()\n",
        "\n",
        "    # Resize to 32x32x32\n",
        "    zoom_factors = [32 / s for s in region_crop.shape]\n",
        "    region_resized = zoom(region_crop, zoom=zoom_factors, order=1)\n",
        "    print(f\"  Resized shape: {region_resized.shape}\")\n",
        "\n",
        "    # Save with format: region_folder/subjectID_label.npy\n",
        "    save_path = os.path.join(save_dir, region_name, f\"{subject_id}_{label}.npy\")\n",
        "    np.save(save_path, region_resized)\n",
        "    print(f\"✅ Saved to {save_path}\\n---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "EY3zZMTZGE82",
        "outputId": "40f1883e-dc3a-427b-e7b3-cbb8f0fdf382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;34m[\u001b[0m\u001b[34mget_dataset_dir\u001b[0m\u001b[1;34m]\u001b[0m Dataset found in \u001b[35m/root/nilearn_data/\u001b[0m\u001b[95mfsl\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">get_dataset_dir</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Dataset found in <span style=\"color: #800080; text-decoration-color: #800080\">/root/nilearn_data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">fsl</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nilearn/image/resampling.py:805: FutureWarning: From release 0.13.0 onwards, this function will, by default, copy the header of the input image to the output. Currently, the header is reset to the default Nifti1Header. To suppress this warning and use the new behavior, set `copy_header=True`.\n",
            "  return resample_img(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Region 5: Left_Caudate\n",
            "⚠️ Region Left_Caudate is empty. Skipping.\n",
            "---\n",
            "Processing Region 6: Left_Putamen\n",
            "⚠️ Region Left_Putamen is empty. Skipping.\n",
            "---\n",
            "Processing Region 8: Brainstem\n",
            "⚠️ Region Brainstem is empty. Skipping.\n",
            "---\n",
            "Processing Region 12: Right_White_Matter\n",
            "⚠️ Region Right_White_Matter is empty. Skipping.\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Folder you want to download\n",
        "folder_path = \"/content/extracted_regions\"\n",
        "\n",
        "# Zip it\n",
        "shutil.make_archive(\"extracted_regions\", 'zip', folder_path)\n",
        "\n",
        "# Download the zip\n",
        "files.download(\"extracted_regions.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lbQnHTokVIaW",
        "outputId": "eeb15310-4a18-4da6-e952-d417316d91ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_781f62d2-b0ae-43de-8fde-33fb74c5cf1a\", \"extracted_regions.zip\", 2660998)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class BrainRegionDataset(Dataset):\n",
        "    def __init__(self, folder_path):\n",
        "        self.filepaths = []\n",
        "        self.labels = []\n",
        "        for fname in os.listdir(folder_path):\n",
        "            if fname.endswith(\".npy\"):\n",
        "                label = 1 if \"pd\" in fname.lower() else 0\n",
        "                self.filepaths.append(os.path.join(folder_path, fname))\n",
        "                self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filepaths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = np.load(self.filepaths[idx]).astype(np.float32)\n",
        "        x = (x - np.mean(x)) / (np.std(x) + 1e-5)  # normalize again\n",
        "        x = np.expand_dims(x, axis=0)  # Add channel dimension\n",
        "        y = self.labels[idx]\n",
        "        return torch.tensor(x), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Load full dataset\n",
        "data_folder = \"/content/extracted_regions/Brainstem\"\n",
        "dataset = BrainRegionDataset(data_folder)\n",
        "\n",
        "# Train-val split\n",
        "train_idx, val_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, stratify=dataset.labels)\n",
        "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
        "val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4)\n"
      ],
      "metadata": {
        "id": "G_AtmUq7hUOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Simple3DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Simple3DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool3d(2)\n",
        "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool3d(2)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8 * 8, 64)  # update based on input shape\n",
        "        self.fc2 = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)"
      ],
      "metadata": {
        "id": "1EWYHeCwiGO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Simple3DCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def train_model(num_epochs=50):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s6o6XVwiO-a",
        "outputId": "f7342f45-cef8-4531-b25e-97a9e87730a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.9325, Accuracy: 0.5000\n",
            "Epoch 2, Loss: 2.4742, Accuracy: 0.7500\n",
            "Epoch 3, Loss: 2.2329, Accuracy: 0.6875\n",
            "Epoch 4, Loss: 2.0524, Accuracy: 0.7500\n",
            "Epoch 5, Loss: 2.0105, Accuracy: 0.6875\n",
            "Epoch 6, Loss: 1.8745, Accuracy: 0.8125\n",
            "Epoch 7, Loss: 1.5704, Accuracy: 0.8750\n",
            "Epoch 8, Loss: 1.4609, Accuracy: 0.8750\n",
            "Epoch 9, Loss: 1.3067, Accuracy: 0.9375\n",
            "Epoch 10, Loss: 1.3464, Accuracy: 0.9375\n",
            "Epoch 11, Loss: 1.1423, Accuracy: 0.9375\n",
            "Epoch 12, Loss: 1.2437, Accuracy: 0.8125\n",
            "Epoch 13, Loss: 1.2219, Accuracy: 0.8750\n",
            "Epoch 14, Loss: 1.0313, Accuracy: 0.9375\n",
            "Epoch 15, Loss: 0.8721, Accuracy: 0.9375\n",
            "Epoch 16, Loss: 0.9808, Accuracy: 0.9375\n",
            "Epoch 17, Loss: 0.7960, Accuracy: 0.9375\n",
            "Epoch 18, Loss: 0.8982, Accuracy: 0.9375\n",
            "Epoch 19, Loss: 0.8325, Accuracy: 0.9375\n",
            "Epoch 20, Loss: 0.6688, Accuracy: 1.0000\n",
            "Epoch 21, Loss: 0.6810, Accuracy: 0.9375\n",
            "Epoch 22, Loss: 0.6115, Accuracy: 0.9375\n",
            "Epoch 23, Loss: 0.5763, Accuracy: 0.9375\n",
            "Epoch 24, Loss: 0.5962, Accuracy: 1.0000\n",
            "Epoch 25, Loss: 0.5202, Accuracy: 1.0000\n",
            "Epoch 26, Loss: 0.5276, Accuracy: 0.9375\n",
            "Epoch 27, Loss: 0.5236, Accuracy: 1.0000\n",
            "Epoch 28, Loss: 0.4375, Accuracy: 1.0000\n",
            "Epoch 29, Loss: 0.3910, Accuracy: 1.0000\n",
            "Epoch 30, Loss: 0.3821, Accuracy: 1.0000\n",
            "Epoch 31, Loss: 0.3005, Accuracy: 1.0000\n",
            "Epoch 32, Loss: 0.3926, Accuracy: 1.0000\n",
            "Epoch 33, Loss: 0.3061, Accuracy: 1.0000\n",
            "Epoch 34, Loss: 0.3576, Accuracy: 1.0000\n",
            "Epoch 35, Loss: 0.2498, Accuracy: 1.0000\n",
            "Epoch 36, Loss: 0.2603, Accuracy: 1.0000\n",
            "Epoch 37, Loss: 0.2329, Accuracy: 1.0000\n",
            "Epoch 38, Loss: 0.2657, Accuracy: 1.0000\n",
            "Epoch 39, Loss: 0.2140, Accuracy: 1.0000\n",
            "Epoch 40, Loss: 0.2636, Accuracy: 1.0000\n",
            "Epoch 41, Loss: 0.1797, Accuracy: 1.0000\n",
            "Epoch 42, Loss: 0.2463, Accuracy: 1.0000\n",
            "Epoch 43, Loss: 0.2061, Accuracy: 1.0000\n",
            "Epoch 44, Loss: 0.2021, Accuracy: 1.0000\n",
            "Epoch 45, Loss: 0.1501, Accuracy: 1.0000\n",
            "Epoch 46, Loss: 0.1481, Accuracy: 1.0000\n",
            "Epoch 47, Loss: 0.1228, Accuracy: 1.0000\n",
            "Epoch 48, Loss: 0.1128, Accuracy: 1.0000\n",
            "Epoch 49, Loss: 0.1142, Accuracy: 1.0000\n",
            "Epoch 50, Loss: 0.1060, Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    print(f\"Validation Accuracy: {correct / total:.4f}\")\n",
        "\n",
        "evaluate(val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj0DruuqibJ7",
        "outputId": "34540ba7-a9ee-4597-d6be-db172f6d4b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_conf_matrix():\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    ConfusionMatrixDisplay(cm).plot()\n",
        "    plt.show()\n",
        "\n",
        "plot_conf_matrix()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "MFVD7F_SjDCS",
        "outputId": "9c6d3596-da01-47ef-c39e-bba48c335bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAG2CAYAAACNs6TQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKlpJREFUeJzt3Xt0FfW5//HPTjA7gWQHokIIhFuRW8GAUGlKFdKiMbpQDr+KR/EQua1DuQhEUKiLm7f0aFFEEKookR4oKEoU5Gg5KASE6goS6wXQQGwiBKrlSEiUBPbM7w9k120Q9s7snc2eeb/WmrU6k/nOPHRl+eR5vt+ZcZmmaQoAANhCTKQDAAAAoUNiBwDARkjsAADYCIkdAAAbIbEDAGAjJHYAAGyExA4AgI2Q2AEAsBESOwAANkJiBwDARkjsAACEQX5+vn72s58pKSlJLVu21NChQ7V///4LjnvppZfUrVs3xcfHq1evXtq0aVNQ9yWxAwAQBtu2bdPEiRP117/+VZs3b9apU6d0/fXXq6am5kfH7Ny5U7fffrvGjBmjPXv2aOjQoRo6dKg++uijgO/r4iMwAACE35dffqmWLVtq27Ztuvbaa895zm233aaamhpt3LjRd+znP/+5evfurWXLlgV0nyYhiTZCDMPQ4cOHlZSUJJfLFelwAABBMk1TJ06cUFpammJiwtdEPnnypOrq6ixfxzTNevnG7XbL7XZfcOzx48clSSkpKT96zq5du5SXl+d3LDs7W4WFhQHHGNWJ/fDhw0pPT490GAAAiyoqKtS2bduwXPvkyZPq2D5RR/7htXytxMREVVdX+x2bO3eu5s2bd95xhmFo6tSpGjBggHr27Pmj5x05ckStWrXyO9aqVSsdOXIk4BijOrEnJSVJkn6pG9VEl0Q4GgBAsE7rlHZok++/5+FQV1enI//w6u+7O8iT1PCuQNUJQ+37fq6Kigp5PB7f8UCq9YkTJ+qjjz7Sjh07Gnz/QEV1Yj/bDmmiS9TERWIHgKjz3SqvxphOTUxyKTGp4fcxdGasx+PxS+wXMmnSJG3cuFFFRUUX7Eqkpqbq6NGjfseOHj2q1NTUgO/HqngAgCN4TcPyFgzTNDVp0iStX79eb731ljp27HjBMZmZmdqyZYvfsc2bNyszMzPg+0Z1xQ4AQKAMmTLU8AfBgh07ceJErV69Wq+++qqSkpJ88+TJyclKSEiQJI0cOVJt2rRRfn6+JGnKlCkaOHCgFixYoJtuuklr1qxRcXGxnnnmmYDvS8UOAEAYLF26VMePH9egQYPUunVr37Z27VrfOeXl5aqsrPTt/+IXv9Dq1av1zDPPKCMjQ+vWrVNhYeF5F9z9EBU7AMARDBkKrplef3wwAnlNzNatW+sdu/XWW3XrrbcGda/vI7EDABzBa5ryWngnm5WxjYlWPAAANkLFDgBwhMZePBcpJHYAgCMYMuV1QGKnFQ8AgI1QsQMAHIFWPAAANsKqeAAAEHWo2AEAjmB8t1kZHw1I7AAAR/BaXBVvZWxjIrEDABzBa57ZrIyPBsyxAwBgI1TsAABHYI4dAAAbMeSSVy5L46MBrXgAAGyEih0A4AiGeWazMj4akNgBAI7gtdiKtzK2MdGKBwDARqjYAQCO4JSKncQOAHAEw3TJMC2sircwtjHRigcAwEao2AEAjkArHgAAG/EqRl4LjWpvCGMJJxI7AMARTItz7CZz7AAAoLFRsQMAHIE5dgAAbMRrxshrWphjj5JXytKKBwDARqjYAQCOYMglw0I9ayg6SnYSOwDAEZwyx04rHgAAG6FiBwA4gvXFc7TiAQC4aJyZY7fwERha8QAAoLFRsQMAHMGw+K54VsUDAHARYY4dAAAbMRTjiOfYmWMHAMBGqNgBAI7gNV3yWvj0qpWxjYnEDgBwBK/FxXNeWvEAAKCxUbEDABzBMGNkWFgVb7AqHgCAiweteAAAEHWo2AEAjmDI2sp2I3ShhBWJHQDgCNZfUBMdTe7oiBIAAASEih0A4AjW3xUfHbUwiR0A4AhO+R47iR0A4AhOqdijI0oAABAQKnYAgCNYf0FNdNTCJHYAgCMYpkuGlefYo+TrbtHx5wcAAAgIFTsAwBEMi634aHlBDYkdAOAI1r/uFh2JPTqiBAAAAaFiBwA4glcueS28ZMbK2MZEYgcAOAKteAAAEHWo2AEAjuCVtXa6N3ShhBWJHQDgCE5pxZPYAQCOwEdgAABAgxUVFWnIkCFKS0uTy+VSYWHhBcesWrVKGRkZatq0qVq3bq3Ro0frn//8Z1D3JbEDABzB/O577A3dzCDn52tqapSRkaElS5YEdP4777yjkSNHasyYMfr444/10ksv6b333tO4ceOCui+teACAIzR2Kz4nJ0c5OTkBn79r1y516NBBd999tySpY8eO+s///E/913/9V1D3pWIHACAIVVVVflttbW1IrpuZmamKigpt2rRJpmnq6NGjWrdunW688cagrkNiBwA4wtnPtlrZJCk9PV3Jycm+LT8/PyTxDRgwQKtWrdJtt92muLg4paamKjk5OeBW/lm04gEAjuC1+HW3s2MrKirk8Xh8x91ut+XYJOmTTz7RlClTNGfOHGVnZ6uyslIzZszQ+PHj9dxzzwV8HRI7AABB8Hg8fok9VPLz8zVgwADNmDFDknTllVeqWbNmuuaaa/TQQw+pdevWAV2HxA4AcITvt9MbOj6cvvnmGzVp4p+WY2NjJUmmaQZ8HRI7AMARDMXIsNCKD3ZsdXW1SktLfftlZWUqKSlRSkqK2rVrp1mzZunQoUNauXKlJGnIkCEaN26cli5d6mvFT506VVdffbXS0tICvi+JHQCAMCguLlZWVpZvPy8vT5KUm5urgoICVVZWqry83Pfzu+66SydOnNDixYt1zz33qHnz5vrVr34V9ONuJHYAgCN4TZe8FtrpwY4dNGjQeVvoBQUF9Y5NnjxZkydPDjY0PyR2AIAjXOxz7KFCYgcAOIJp8etuJh+BAQAAjY2KHQDgCF655A3yQy4/HB8NSOwAAEcwTGvz5Ebgj5JHFK14AABshIodAenZv1q3TvhSV/T6Rpemnta80R20643kSIcFhAS/385gWFw8Z2VsY7ooolyyZIk6dOig+Ph49e/fX++9916kQ8IPxDc1dPDjeC3+XdtIhwKEHL/fzmDIZXmLBhGv2NeuXau8vDwtW7ZM/fv318KFC5Wdna39+/erZcuWkQ4P3yl+26Pit0P/0QPgYsDvN+wk4hX7448/rnHjxmnUqFHq0aOHli1bpqZNm+r555+PdGgAABs5++Y5K1s0iGhir6ur0+7duzV48GDfsZiYGA0ePFi7du2KYGQAALs5O8duZYsGEW3Ff/XVV/J6vWrVqpXf8VatWmnfvn31zq+trVVtba1vv6qqKuwxAgAQTaLjz4/v5OfnKzk52belp6dHOiQAQJQw5PK9L75BW5QsnotoYr/ssssUGxuro0eP+h0/evSoUlNT650/a9YsHT9+3LdVVFQ0VqgAgChnWlwRb5LYLywuLk59+/bVli1bfMcMw9CWLVuUmZlZ73y32y2Px+O3oXHEN/Wq00+/VaeffitJSk2vU6effqvL29RFODLAOn6/ncFStW7xy3CNKeKPu+Xl5Sk3N1f9+vXT1VdfrYULF6qmpkajRo2KdGj4ni4Z3+qxlw/49sfPPyxJ+svaFlowrV2kwgJCgt9v2EnEE/ttt92mL7/8UnPmzNGRI0fUu3dvvfHGG/UW1CGy/rYrUdlpGZEOAwgLfr+dwSlvnot4YpekSZMmadKkSZEOAwBgY1bb6dHSio+OPz8AAEBALoqKHQCAcLP6vvdoedyNxA4AcARa8QAAIOpQsQMAHMEpFTuJHQDgCE5J7LTiAQCwESp2AIAjOKViJ7EDABzBlLVH1szQhRJWJHYAgCM4pWJnjh0AABuhYgcAOIJTKnYSOwDAEZyS2GnFAwBgI1TsAABHcErFTmIHADiCabpkWkjOVsY2JlrxAADYCBU7AMAR+B47AAA24pQ5dlrxAADYCBU7AMARnLJ4jsQOAHAEp7TiSewAAEdwSsXOHDsAADZCxQ4AcATTYis+Wip2EjsAwBFMSaZpbXw0oBUPAICNULEDABzBkEsu3jwHAIA9sCoeAABEHSp2AIAjGKZLLl5QAwCAPZimxVXxUbIsnlY8AAA2QsUOAHAEpyyeI7EDAByBxA4AgI04ZfEcc+wAANgIFTsAwBGcsiqexA4AcIQzid3KHHsIgwkjWvEAANgIFTsAwBFYFQ8AgI2YsvZN9SjpxNOKBwDATqjYAQCOQCseAAA7cUgvnlY8AMAZvqvYG7opyIq9qKhIQ4YMUVpamlwulwoLCy84pra2Vvfff7/at28vt9utDh066Pnnnw/qvlTsAACEQU1NjTIyMjR69GgNGzYsoDHDhw/X0aNH9dxzz6lz586qrKyUYRhB3ZfEDgBwhMZ+81xOTo5ycnICPv+NN97Qtm3bdPDgQaWkpEiSOnToENxNRSseAOAQVtrw3194V1VV5bfV1taGJL7XXntN/fr106OPPqo2bdqoS5cumj59ur799tugrkPFDgBAENLT0/32586dq3nz5lm+7sGDB7Vjxw7Fx8dr/fr1+uqrrzRhwgT985//1IoVKwK+DokdAOAMDVgAV2+8pIqKCnk8Ht9ht9ttNTJJkmEYcrlcWrVqlZKTkyVJjz/+uH7zm9/o6aefVkJCQkDXIbEDABwhVHPsHo/HL7GHSuvWrdWmTRtfUpek7t27yzRNffHFF7riiisCug5z7AAAXAQGDBigw4cPq7q62nfs008/VUxMjNq2bRvwdUjsAABnMEOwBaG6ulolJSUqKSmRJJWVlamkpETl5eWSpFmzZmnkyJG+8++44w5deumlGjVqlD755BMVFRVpxowZGj16dMBteCnAVvxrr70W8AVvvvnmgM8FAKCxNPYrZYuLi5WVleXbz8vLkyTl5uaqoKBAlZWVviQvSYmJidq8ebMmT56sfv366dJLL9Xw4cP10EMPBXXfgBL70KFDA7qYy+WS1+sNKgAAAOxo0KBBMs8zqV9QUFDvWLdu3bR582ZL9w0osQf71hsAAC5KUfK+dyssrYo/efKk4uPjQxULAABh45SvuwW9eM7r9erBBx9UmzZtlJiYqIMHD0qSZs+ereeeey7kAQIAEBKNvHguUoJO7A8//LAKCgr06KOPKi4uzne8Z8+eWr58eUiDAwAAwQk6sa9cuVLPPPOMRowYodjYWN/xjIwM7du3L6TBAQAQOq4QbBe/oOfYDx06pM6dO9c7bhiGTp06FZKgAAAIOavtdLu24nv06KHt27fXO75u3Tr16dMnJEEBAICGCbpinzNnjnJzc3Xo0CEZhqFXXnlF+/fv18qVK7Vx48ZwxAgAgHVU7Od2yy23aMOGDfrf//1fNWvWTHPmzNHevXu1YcMGXXfddeGIEQAA685+3c3KFgUa9Bz7NddcY/nNOAAAIPQa/IKa4uJi7d27V9KZefe+ffuGLCgAAEItVJ9tvdgFndi/+OIL3X777XrnnXfUvHlzSdLXX3+tX/ziF1qzZk1Qn5YDAKDRMMd+bmPHjtWpU6e0d+9eHTt2TMeOHdPevXtlGIbGjh0bjhgBAECAgq7Yt23bpp07d6pr166+Y127dtVTTz2la665JqTBAQAQMlYXwNl18Vx6evo5X0Tj9XqVlpYWkqAAAAg1l3lmszI+GgTdin/sscc0efJkFRcX+44VFxdrypQp+sMf/hDS4AAACBmHfAQmoIq9RYsWcrn+1YKoqalR//791aTJmeGnT59WkyZNNHr0aA0dOjQsgQIAgAsLKLEvXLgwzGEAABBmzLH/S25ubrjjAAAgvBzyuFuDX1AjSSdPnlRdXZ3fMY/HYykgAADQcEEvnqupqdGkSZPUsmVLNWvWTC1atPDbAAC4KDlk8VzQif3ee+/VW2+9paVLl8rtdmv58uWaP3++0tLStHLlynDECACAdQ5J7EG34jds2KCVK1dq0KBBGjVqlK655hp17txZ7du316pVqzRixIhwxAkAAAIQdMV+7NgxderUSdKZ+fRjx45Jkn75y1+qqKgotNEBABAqDvlsa9CJvVOnTiorK5MkdevWTS+++KKkM5X82Y/CAABwsTn75jkrWzQIOrGPGjVKH3zwgSRp5syZWrJkieLj4zVt2jTNmDEj5AECAIDABT3HPm3aNN//Hjx4sPbt26fdu3erc+fOuvLKK0MaHAAAIcNz7IFp37692rdvH4pYAACARQEl9kWLFgV8wbvvvrvBwQAAEC4uWfy6W8giCa+AEvsTTzwR0MVcLheJHQCACAoosZ9dBX+xWv/ph/IkBb0OEIgK2Wm9Ix0CYA98BAYAABtxyOI5ylwAAGyEih0A4AwOqdhJ7AAAR7D69jjbvnkOAABcvBqU2Ldv364777xTmZmZOnTokCTpT3/6k3bs2BHS4AAACBmHfLY16MT+8ssvKzs7WwkJCdqzZ49qa2slScePH9cjjzwS8gABAAgJEvu5PfTQQ1q2bJmeffZZXXLJJb7jAwYM0Pvvvx/S4AAAQHCCXjy3f/9+XXvttfWOJycn6+uvvw5FTAAAhByL535EamqqSktL6x3fsWOHOnXqFJKgAAAIubNvnrOyRYGgE/u4ceM0ZcoUvfvuu3K5XDp8+LBWrVql6dOn67e//W04YgQAwDqHzLEH3YqfOXOmDMPQr3/9a33zzTe69tpr5Xa7NX36dE2ePDkcMQIAgAAFndhdLpfuv/9+zZgxQ6WlpaqurlaPHj2UmJgYjvgAAAgJp8yxN/jNc3FxcerRo0coYwEAIHx4pey5ZWVlyeX68QUEb731lqWAAABAwwWd2Hv37u23f+rUKZWUlOijjz5Sbm5uqOICACC0LLbibVuxP/HEE+c8Pm/ePFVXV1sOCACAsHBIKz5kH4G588479fzzz4fqcgAAoAFC9tnWXbt2KT4+PlSXAwAgtBxSsQed2IcNG+a3b5qmKisrVVxcrNmzZ4csMAAAQonH3X5EcnKy335MTIy6du2qBx54QNdff33IAgMAAMELKrF7vV6NGjVKvXr1UosWLcIVEwAAaKCgFs/Fxsbq+uuv5ytuAIDo45B3xQe9Kr5nz546ePBgOGIBACBszs6xW9miQdCJ/aGHHtL06dO1ceNGVVZWqqqqym8DAACRE/Ac+wMPPKB77rlHN954oyTp5ptv9nu1rGmacrlc8nq9oY8SAIBQiJKq24qAE/v8+fM1fvx4vf322+GMBwCA8OA5dn+meeZfNHDgwLAFAwAArAnqcbfzfdUNAICLGS+oOYcuXbpcMLkfO3bMUkAAAIQFrfj65s+fX+/NcwAA4OIRVGL/93//d7Vs2TJcsQAAEDZOacUH/Bw78+sAgKjWyG+eKyoq0pAhQ5SWliaXy6XCwsKAx77zzjtq0qSJevfuHdxNFURiP7sqHgAAXFhNTY0yMjK0ZMmSoMZ9/fXXGjlypH7961836L4Bt+INw2jQDQAAuCg08uK5nJwc5eTkBH2b8ePH64477lBsbGxQVf5ZQb9SFgCAaBSqd8X/8FXqtbW1IYtxxYoVOnjwoObOndvga5DYAQDOEKI59vT0dCUnJ/u2/Pz8kIT32WefaebMmfrv//5vNWkS1Np2Pw0fCQCAA1VUVMjj8fj23W635Wt6vV7dcccdmj9/vrp06WLpWiR2AIAzhGiO3ePx+CX2UDhx4oSKi4u1Z88eTZo0SdKZtW2maapJkyb6y1/+ol/96lcBXYvEDgBwhIv5OXaPx6MPP/zQ79jTTz+tt956S+vWrVPHjh0DvhaJHQCAMKiurlZpaalvv6ysTCUlJUpJSVG7du00a9YsHTp0SCtXrlRMTIx69uzpN75ly5aKj4+vd/xCSOwAAGdo5MfdiouLlZWV5dvPy8uTJOXm5qqgoECVlZUqLy+3ENC5kdgBAI7Q2K34QYMGnfflbgUFBecdP2/ePM2bNy+4m4rH3QAAsBUqdgCAM/DZVgAAbMQhiZ1WPAAANkLFDgBwBNd3m5Xx0YDEDgBwBoe04knsAABHuJjfPBdKzLEDAGAjVOwAAGegFQ8AgM1ESXK2glY8AAA2QsUOAHAEpyyeI7EDAJzBIXPstOIBALARKnYAgCPQigcAwE5oxQMAgGhDxQ4AcARa8QAA2IlDWvEkdgCAMzgksTPHDgCAjVCxAwAcgTl2AADshFY8AACINlTsAABHcJmmXGbDy24rYxsTiR0A4Ay04gEAQLShYgcAOAKr4gEAsBNa8QAAINpQsQMAHIFWPAAAduKQVjyJHQDgCE6p2JljBwDARqjYAQDOQCseAAB7iZZ2uhW04gEAsBEqdgCAM5jmmc3K+ChAYgcAOAKr4gEAQNShYgcAOAOr4gEAsA+XcWazMj4a0IoHAMBGqNhxQWueaql3NjVXRalbcfGGevT7RmPuP6z0zrWRDg0IiZ79q3XrhC91Ra9vdGnqac0b3UG73kiOdFgINYe04iNasRcVFWnIkCFKS0uTy+VSYWFhJMPBj/jbrkQNuesrLdz4mfLXHJD3tPS723+ik9/Q8IE9xDc1dPDjeC3+XdtIh4IwOrsq3soWDSJasdfU1CgjI0OjR4/WsGHDIhkKzuOR1Qf99u9ZWK7bevXSZ39LUK+f10QoKiB0it/2qPhtT6TDQLjxHHv45eTkKCcnJ5IhoAFqqmIlSUnNvRGOBADwQ1E1x15bW6va2n/N61ZVVUUwGmcyDGnZ3Db66c+q1aHbyUiHAwAB4wU1F6H8/HwlJyf7tvT09EiH5DiLf9dWf9+XoFlL/x7pUAAgOGYItigQVYl91qxZOn78uG+rqKiIdEiOsvh3bfTuZo8eXVeqy9NORTocAMA5RFUr3u12y+12RzoMxzFNacn9bbTzjWQ9tq5Uqe3qIh0SAATNKa34qErsiIzFv2urt9e30LwVB5WQaOjYP8782jRL8sqdECW/6cB5xDf1Kq3jv/5gTU2vU6effqsTX8fqy0NxEYwMIcWq+PCrrq5WaWmpb7+srEwlJSVKSUlRu3btIhgZvm/jC5dJkmb8vyv8jt/zRLmuv+1YJEICQqpLxrd67OUDvv3x8w9Lkv6ytoUWTOO/RYguEU3sxcXFysrK8u3n5eVJknJzc1VQUBChqPBDbx4uiXQIQFj9bVeistMyIh0GwoxWfCMYNGiQzChpbQAAohyvlAUAANGGxXMAAEegFQ8AgJ0Y5pnNyvgoQGIHADgDc+wAACDaULEDABzBJYtz7CGLJLxI7AAAZ3DIm+doxQMAYCMkdgCAI5x93M3KFoyioiINGTJEaWlpcrlcKiwsPO/5r7zyiq677jpdfvnl8ng8yszM1Jtvvhn0v5PEDgBwhkb+HntNTY0yMjK0ZMmSgM4vKirSddddp02bNmn37t3KysrSkCFDtGfPnqDuyxw7AABhkJOTo5ycnIDPX7hwod/+I488oldffVUbNmxQnz59Ar4OiR0A4Agu05TLwgK4s2Orqqr8jrvdbrndbkuxnYthGDpx4oRSUlKCGkcrHgDgDEYINknp6elKTk72bfn5+WEJ9w9/+IOqq6s1fPjwoMZRsQMAEISKigp5PB7ffjiq9dWrV2v+/Pl69dVX1bJly6DGktgBAI4Qqla8x+PxS+yhtmbNGo0dO1YvvfSSBg8eHPR4EjsAwBmi4F3xf/7znzV69GitWbNGN910U4OuQWIHADhDI795rrq6WqWlpb79srIylZSUKCUlRe3atdOsWbN06NAhrVy5UtKZ9ntubq6efPJJ9e/fX0eOHJEkJSQkKDk5OeD7sngOAIAwKC4uVp8+fXyPquXl5alPnz6aM2eOJKmyslLl5eW+85955hmdPn1aEydOVOvWrX3blClTgrovFTsAwBEa8va4H44PxqBBg2Sep8ovKCjw29+6dWvwQZ0DiR0A4Ax8BAYAAEQbKnYAgCO4jDOblfHRgMQOAHAGWvEAACDaULEDAJwhCl5QEwokdgCAI4TqlbIXO1rxAADYCBU7AMAZHLJ4jsQOAHAGU75vqjd4fBQgsQMAHIE5dgAAEHWo2AEAzmDK4hx7yCIJKxI7AMAZHLJ4jlY8AAA2QsUOAHAGQ5LL4vgoQGIHADgCq+IBAEDUoWIHADiDQxbPkdgBAM7gkMROKx4AABuhYgcAOINDKnYSOwDAGXjcDQAA++BxNwAAEHWo2AEAzsAcOwAANmKYkstCcjaiI7HTigcAwEao2AEAzkArHgAAO7GY2BUdiZ1WPAAANkLFDgBwBlrxAADYiGHKUjudVfEAAKCxUbEDAJzBNM5sVsZHARI7AMAZmGMHAMBGmGMHAADRhoodAOAMtOIBALARUxYTe8giCSta8QAA2AgVOwDAGWjFAwBgI4YhycKz6EZ0PMdOKx4AABuhYgcAOAOteAAAbMQhiZ1WPAAANkLFDgBwBoe8UpbEDgBwBNM0ZFr4QpuVsY2JxA4AcAbTtFZ1M8cOAAAaGxU7AMAZTItz7FFSsZPYAQDOYBiSy8I8eZTMsdOKBwDARqjYAQDOQCseAAD7MA1DpoVWfLQ87kYrHgAAG6FiBwA4A614AABsxDAll/0TO614AABshIodAOAMpinJynPs0VGxk9gBAI5gGqZMC614k8QOAMBFxDRkrWLncTcAAByrqKhIQ4YMUVpamlwulwoLCy84ZuvWrbrqqqvkdrvVuXNnFRQUBH1fEjsAwBFMw7S8BaOmpkYZGRlasmRJQOeXlZXppptuUlZWlkpKSjR16lSNHTtWb775ZlD3pRUPAHCGRm7F5+TkKCcnJ+Dzly1bpo4dO2rBggWSpO7du2vHjh164oknlJ2dHfB1ojqxn13IUFUdHfMeQEOcNk9FOgQgbE7rzO93YyxMO61Tlt5PczbWqqoqv+Nut1tut9tKaJKkXbt2afDgwX7HsrOzNXXq1KCuE9WJ/cSJE5Kk9ld9HtlAgLA6GOkAgLA7ceKEkpOTw3LtuLg4paamaseRTZavlZiYqPT0dL9jc+fO1bx58yxf+8iRI2rVqpXfsVatWqmqqkrffvutEhISArpOVCf2tLQ0VVRUKCkpSS6XK9LhOEJVVZXS09NVUVEhj8cT6XCAkOL3u/GZpqkTJ04oLS0tbPeIj49XWVmZ6urqLF/LNM16+SYU1XooRXVij4mJUdu2bSMdhiN5PB7+wwfb4ve7cYWrUv+++Ph4xcfHh/0+VqSmpuro0aN+x44ePSqPxxNwtS6xKh4AgItCZmamtmzZ4nds8+bNyszMDOo6JHYAAMKgurpaJSUlKikpkXTmcbaSkhKVl5dLkmbNmqWRI0f6zh8/frwOHjyoe++9V/v27dPTTz+tF198UdOmTQvqviR2BMXtdmvu3LkX3ZwSEAr8fiOUiouL1adPH/Xp00eSlJeXpz59+mjOnDmSpMrKSl+Sl6SOHTvq9ddf1+bNm5WRkaEFCxZo+fLlQT3qJkkuM1pefgsAAC6Iih0AABshsQMAYCMkdgAAbITEDgCAjZDYEbAlS5aoQ4cOio+PV//+/fXee+9FOiQgJBryeU3gYkViR0DWrl2rvLw8zZ07V++//74yMjKUnZ2tf/zjH5EODbAs2M9rAhczHndDQPr376+f/exnWrx4sSTJMAylp6dr8uTJmjlzZoSjA0LH5XJp/fr1Gjp0aKRDARqEih0XVFdXp927d/t9TjAmJkaDBw/Wrl27IhgZAOCHSOy4oK+++kper/ecnxM8cuRIhKICAJwLiR0AABshseOCLrvsMsXGxp7zc4KpqakRigoAcC4kdlxQXFyc+vbt6/c5QcMwtGXLlqA/JwgACK8mkQ4A0SEvL0+5ubnq16+frr76ai1cuFA1NTUaNWpUpEMDLKuurlZpaalv/+znNVNSUtSuXbsIRgYEj8fdELDFixfrscce05EjR9S7d28tWrRI/fv3j3RYgGVbt25VVlZWveO5ubkqKCho/IAAC0jsAADYCHPsAADYCIkdAAAbIbEDAGAjJHYAAGyExA4AgI2Q2AEAsBESOwAANkJiByy66667/L7dPWjQIE2dOrXR49i6datcLpe+/vrrHz3H5XKpsLAw4GvOmzdPvXv3thTX559/LpfLpZKSEkvXARAYEjts6a677pLL5ZLL5VJcXJw6d+6sBx54QKdPnw77vV955RU9+OCDAZ0bSDIGgGDwrnjY1g033KAVK1aotrZWmzZt0sSJE3XJJZdo1qxZ9c6tq6tTXFxcSO6bkpISkusAQENQscO23G63UlNT1b59e/32t7/V4MGD9dprr0n6V/v84YcfVlpamrp27SpJqqio0PDhw9W8eXOlpKTolltu0eeff+67ptfrVV5enpo3b65LL71U9957r374VuYftuJra2t13333KT09XW63W507d9Zzzz2nzz//3Pd+8hYtWsjlcumuu+6SdObrefn5+erYsaMSEhKUkZGhdevW+d1n06ZN6tKlixISEpSVleUXZ6Duu+8+denSRU2bNlWnTp00e/ZsnTp1qt55f/zjH5Wenq6mTZtq+PDhOn78uN/Ply9fru7duys+Pl7dunXT008/HXQsAEKDxA7HSEhIUF1dnW9/y5Yt2r9/vzZv3qyNGzfq1KlTys7OVlJSkrZv36533nlHiYmJuuGGG3zjFixYoIKCAj3//PPasWOHjh07pvXr15/3viNHjtSf//xnLVq0SHv37tUf//hHJSYmKj09XS+//LIkaf/+/aqsrNSTTz4pScrPz9fKlSu1bNkyffzxx5o2bZruvPNObdu2TdKZP0CGDRumIUOGqKSkRGPHjtXMmTOD/v8kKSlJBQUF+uSTT/Tkk0/q2Wef1RNPPOF3TmlpqV588UVt2LBBb7zxhvbs2aMJEyb4fr5q1SrNmTNHDz/8sPbu3atHHnlEs2fP1gsvvBB0PABCwARsKDc317zllltM0zRNwzDMzZs3m26325w+fbrv561atTJra2t9Y/70pz+ZXbt2NQ3D8B2rra01ExISzDfffNM0TdNs3bq1+eijj/p+furUKbNt27a+e5mmaQ4cONCcMmWKaZqmuX//flOSuXnz5nPG+fbbb5uSzP/7v//zHTt58qTZtGlTc+fOnX7njhkzxrz99ttN0zTNWbNmmT169PD7+X333VfvWj8kyVy/fv2P/vyxxx4z+/bt69ufO3euGRsba37xxRe+Y//zP/9jxsTEmJWVlaZpmuZPfvITc/Xq1X7XefDBB83MzEzTNE2zrKzMlGTu2bPnR+8LIHSYY4dtbdy4UYmJiTp16pQMw9Add9yhefPm+X7eq1cvv3n1Dz74QKWlpUpKSvK7zsmTJ3XgwAEdP35clZWVfp+qbdKkifr161evHX9WSUmJYmNjNXDgwIDjLi0t1TfffKPrrrvO73hdXZ369OkjSdq7d2+9T+ZmZmYGfI+z1q5dq0WLFunAgQOqrq7W6dOn5fF4/M5p166d2rRp43cfwzC0f/9+JSUl6cCBAxozZozGjRvnO+f06dNKTk4OOh4A1pHYYVtZWVlaunSp4uLilJaWpiZN/H/dmzVr5rdfXV2tvn37atWqVfWudfnllzcohoSEhKDHVFdXS5Jef/11v4QqnVk3ECq7du3SiBEjNH/+fGVnZys5OVlr1qzRggULgo712WefrfeHRmxsbMhiBRA4Ejtsq1mzZurcuXPA51911VVau3atWrZsWa9qPat169Z69913de2110o6U5nu3r1bV1111TnP79WrlwzD0LZt2zR48OB6Pz/bMfB6vb5jPXr0kNvtVnl5+Y9W+t27d/ctBDzrr3/964X/kd+zc+dOtW/fXvfff7/v2N///vd655WXl+vw4cNKS0vz3ScmJkZdu3ZVq1atlJaWpoMHD2rEiBFB3R9AeLB4DvjOiBEjdNlll+mWW27R9u3bVVZWpq1bt+ruu+/WF198IUmaMmWKfv/736uwsFD79u3ThAkTzvsMeocOHZSbm6vRo0ersLDQd80XX3xRktS+fXu5XC5t3LhRX375paqrq5WUlKTp06dr2rRpeuGFF3TgwAG9//77euqpp3wL0saPH6/PPvtMM2bM0P79+7V69WoVFBQE9e+94oorVF5erjVr1ujAgQNatGjRORcCxsfHKzc3Vx988IG2b9+uu+++W8OHD1dqaqokaf78+crPz9eiRYv06aef6sMPP9SKFSv0+OOPBxUPgNAgsQPfadq0qYqKitSuXTsNGzZM3bt315gxY3Ty5ElfBX/PPffoP/7jP5Sbm6vMzEwlJSXp3/7t38573aVLl+o3v/mNJkyYoG7dumncuHGqqamRJLVp00bz58/XzJkz1apVK02aNEmS9OCDD2r27NnKz89X9+7ddcMNN+j1119Xx44dJZ2Z93755ZdVWFiojIwMLVu2TI888khQ/96bb75Z06ZN06RJk9S7d2/t3LlTs2fPrnde586dNWzYMN144426/vrrdeWVV/o9zjZ27FgtX75cK1asUK9evTRw4EAVFBT4YgXQuFzmj636AQAAUYeKHQAAGyGxAwBgIyR2AABshMQOAICNkNgBALAREjsAADZCYgcAwEZI7AAA2AiJHQAAGyGxAwBgIyR2AABshMQOAICN/H/0QL75yhzVXwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "region_dirs = [\"Brainstem\", \"Left_Caudate\", \"Left_Putamen\", \"Right_White_Matter\"]\n",
        "\n",
        "class MultiRegionDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.samples = []\n",
        "        all_ids = set()\n",
        "        for region in region_dirs:\n",
        "            for file in os.listdir(os.path.join(root_dir, region)):\n",
        "                subj_id = \"_\".join(file.split(\"_\")[:-1])\n",
        "                all_ids.add(subj_id)\n",
        "        for subj_id in all_ids:\n",
        "            paths = []\n",
        "            label = None\n",
        "            complete = True\n",
        "            for region in region_dirs:\n",
        "                path = os.path.join(root_dir, region, f\"{subj_id}_0.npy\")\n",
        "                alt_path = os.path.join(root_dir, region, f\"{subj_id}_1.npy\")\n",
        "                if os.path.exists(path):\n",
        "                    paths.append(path)\n",
        "                    label = 0\n",
        "                elif os.path.exists(alt_path):\n",
        "                    paths.append(alt_path)\n",
        "                    label = 1\n",
        "                else:\n",
        "                    complete = False\n",
        "            if complete:\n",
        "                self.samples.append((paths, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        paths, label = self.samples[idx]\n",
        "        regions = [torch.tensor(np.load(p), dtype=torch.float32).unsqueeze(0) for p in paths]\n",
        "        return torch.stack(regions), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "class Simple3DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Simple3DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1, 8, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(8)\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv3d(8, 16, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm3d(16)\n",
        "\n",
        "        # Ensure fixed size output using Adaptive pooling\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool3d((2, 2, 2))  # Output shape: (B, 16, 2, 2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 2 * 2 * 2, 64)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.out = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.out(x)\n",
        "\n",
        "class MultiRegionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiRegionModel, self).__init__()\n",
        "        self.region_models = nn.ModuleList([Simple3DCNN() for _ in range(4)])\n",
        "        self.classifier = nn.Linear(64 * 4, 2)\n",
        "\n",
        "    def forward(self, x):  # x shape: [B, 4, 1, D, H, W]\n",
        "        outputs = []\n",
        "        for i in range(4):\n",
        "            out = self.region_models[i](x[:, i])  # Pass each region\n",
        "            outputs.append(out)\n",
        "        combined = torch.cat(outputs, dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# === Training Setup ===\n",
        "root_dir = \"/content/drive/MyDrive/extracted_regions1\"\n",
        "dataset = MultiRegionDataset(root_dir)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=4)\n",
        "\n",
        "model = MultiRegionModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# === Training ===\n",
        "for epoch in range(1, 51):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += (output.argmax(1) == y).sum().item()\n",
        "    acc = correct / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch}, Loss: {total_loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# === Validation (optional) ===\n",
        "model.eval()\n",
        "val_correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        output = model(x)\n",
        "        val_correct += (output.argmax(1) == y).sum().item()\n",
        "val_acc = val_correct / len(val_loader.dataset)\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "EizFRKWe9oRB",
        "outputId": "1d3fadf4-5f1b-4cc2-bd37-d65a7e11fdb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (4x8 and 256x2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-de438ba3c37d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-de438ba3c37d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# === Training Setup ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x8 and 256x2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ------------------------------\n",
        "# Step 1: Dataset Class\n",
        "# ------------------------------\n",
        "class BrainRegionDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.region_dirs = [\"Brainstem\", \"Left_Caudate\", \"Left_Putamen\", \"Right_White_Matter\"]\n",
        "\n",
        "        for region in self.region_dirs:\n",
        "            region_path = os.path.join(root_dir, region)\n",
        "            for fname in os.listdir(region_path):\n",
        "                if fname.endswith('.npy'):\n",
        "                    label = int(fname.split('_')[-1].split('.')[0])  # subject1_0.npy → 0\n",
        "                    self.samples.append({\n",
        "                        'path': os.path.join(region_path, fname),\n",
        "                        'label': label\n",
        "                    })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        data = np.load(sample['path'])\n",
        "        data = torch.tensor(data, dtype=torch.float32).unsqueeze(0)  # shape: (1, D, H, W)\n",
        "        label = torch.tensor(sample['label'], dtype=torch.long)\n",
        "        return data, label\n",
        "\n",
        "# ------------------------------\n",
        "# Step 2: 3D CNN Model\n",
        "# ------------------------------\n",
        "class Improved3DCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Improved3DCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1, 8, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm3d(8)\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv3d(8, 16, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm3d(16)\n",
        "\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool3d((2, 2, 2))  # (B, 16, 2, 2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 2 * 2 * 2, 64)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.out = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.out(x)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 3: Load Data\n",
        "# ------------------------------\n",
        "dataset = BrainRegionDataset(\"/content/drive/MyDrive/extracted_regions1\")\n",
        "train_idx, val_idx = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42, stratify=[d[1].item() for d in dataset])\n",
        "train_set = torch.utils.data.Subset(dataset, train_idx)\n",
        "val_set = torch.utils.data.Subset(dataset, val_idx)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=4)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 4: Train Model\n",
        "# ------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Improved3DCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1, 51):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        preds = output.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Epoch {epoch}, Loss: {running_loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# ------------------------------\n",
        "# Step 5: Validate\n",
        "# ------------------------------\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        output = model(x)\n",
        "        preds = output.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "val_acc = correct / total\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aeukVQ9_5Ku",
        "outputId": "b31a66fc-ba96-4430-8097-b609ee0cdb2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 12.1861, Accuracy: 0.4627\n",
            "Epoch 2, Loss: 11.9777, Accuracy: 0.4478\n",
            "Epoch 3, Loss: 11.7796, Accuracy: 0.5075\n",
            "Epoch 4, Loss: 11.8154, Accuracy: 0.4776\n",
            "Epoch 5, Loss: 11.5683, Accuracy: 0.5075\n",
            "Epoch 6, Loss: 11.4611, Accuracy: 0.5522\n",
            "Epoch 7, Loss: 11.7265, Accuracy: 0.6119\n",
            "Epoch 8, Loss: 11.4777, Accuracy: 0.5672\n",
            "Epoch 9, Loss: 11.0236, Accuracy: 0.6567\n",
            "Epoch 10, Loss: 11.5959, Accuracy: 0.5821\n",
            "Epoch 11, Loss: 11.6451, Accuracy: 0.5373\n",
            "Epoch 12, Loss: 10.7084, Accuracy: 0.6269\n",
            "Epoch 13, Loss: 10.1231, Accuracy: 0.7313\n",
            "Epoch 14, Loss: 10.6875, Accuracy: 0.6269\n",
            "Epoch 15, Loss: 11.9316, Accuracy: 0.5821\n",
            "Epoch 16, Loss: 10.6909, Accuracy: 0.6716\n",
            "Epoch 17, Loss: 11.2489, Accuracy: 0.6418\n",
            "Epoch 18, Loss: 10.9609, Accuracy: 0.6418\n",
            "Epoch 19, Loss: 10.8632, Accuracy: 0.6866\n",
            "Epoch 20, Loss: 10.7234, Accuracy: 0.6418\n",
            "Epoch 21, Loss: 11.7191, Accuracy: 0.5821\n",
            "Epoch 22, Loss: 10.5977, Accuracy: 0.6119\n",
            "Epoch 23, Loss: 10.8921, Accuracy: 0.6269\n",
            "Epoch 24, Loss: 11.4914, Accuracy: 0.5970\n",
            "Epoch 25, Loss: 10.9851, Accuracy: 0.5672\n",
            "Epoch 26, Loss: 10.4970, Accuracy: 0.6119\n",
            "Epoch 27, Loss: 10.3732, Accuracy: 0.6269\n",
            "Epoch 28, Loss: 10.5852, Accuracy: 0.6269\n",
            "Epoch 29, Loss: 10.3450, Accuracy: 0.6269\n",
            "Epoch 30, Loss: 10.6566, Accuracy: 0.7164\n",
            "Epoch 31, Loss: 10.8178, Accuracy: 0.5970\n",
            "Epoch 32, Loss: 9.2603, Accuracy: 0.7313\n",
            "Epoch 33, Loss: 11.0422, Accuracy: 0.6269\n",
            "Epoch 34, Loss: 11.0594, Accuracy: 0.5970\n",
            "Epoch 35, Loss: 10.5854, Accuracy: 0.6269\n",
            "Epoch 36, Loss: 10.5363, Accuracy: 0.6866\n",
            "Epoch 37, Loss: 10.1930, Accuracy: 0.7164\n",
            "Epoch 38, Loss: 11.1950, Accuracy: 0.6119\n",
            "Epoch 39, Loss: 10.6985, Accuracy: 0.5970\n",
            "Epoch 40, Loss: 9.9674, Accuracy: 0.6716\n",
            "Epoch 41, Loss: 11.1698, Accuracy: 0.5821\n",
            "Epoch 42, Loss: 10.2315, Accuracy: 0.6567\n",
            "Epoch 43, Loss: 10.0442, Accuracy: 0.6567\n",
            "Epoch 44, Loss: 10.1960, Accuracy: 0.6716\n",
            "Epoch 45, Loss: 9.4251, Accuracy: 0.7015\n",
            "Epoch 46, Loss: 10.1644, Accuracy: 0.7015\n",
            "Epoch 47, Loss: 9.6554, Accuracy: 0.6866\n",
            "Epoch 48, Loss: 9.8237, Accuracy: 0.6866\n",
            "Epoch 49, Loss: 10.9788, Accuracy: 0.6119\n",
            "Epoch 50, Loss: 10.0577, Accuracy: 0.6418\n",
            "Validation Accuracy: 0.5882\n"
          ]
        }
      ]
    }
  ]
}